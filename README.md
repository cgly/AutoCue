# AutoCue


the LLM-BFF output demo(We only use the templates and prompt words provided by LMM-BFF, so LMM-BFF and adversarial attack code are independent. LM-BFF only needs to be trained on the same distributed datasets.)

```
0.93750 *cls**sent_0*_Read_more_at*mask*_Week._com.*sep+*	{0: "Religion", 1: "Sports", 2: "Business", 3: "Tech"}
0.92188 *cls**sent_0*_You_are_currently_browsing_the*mask*_News_weblog_archives.*sep+*	{0: "Religion", 1: "Sports", 2: "Business", 3: "Tech"}
0.92188 *cls**sent_0*_Read_the_full_story_on*mask*.com.*sep+*	{0: "military", 1: "sport", 2: "Market", 3: "Tech"}
0.90625 *cls**sent_0*_This_article_originally_appeared_on*mask*_News.*sep+*	{0: "Jerusalem", 1: "Sport", 2: "Investment", 3: "Technology"}
0.90625 *cls**sent_0*_This_story_was_originally_published_by*mask*_News.*sep+*	{0: "Al", 1: "Sports", 2: "Investment", 3: "Digital"}
0.90625 *cls**sent_0*_Read_the_full_story_at*mask*.com.*sep+*	{0: "military", 1: "Sports", 2: "investors", 3: "Tech"}
0.90625 *cls**sent_0*_Read_the_full_story_at*mask*.com.*sep+*	{0: "military", 1: "Stadium", 2: "Business", 3: "Tech"}
0.90625 *cls**sent_0*_You_are_currently_browsing_the*mask*_News_weblog_archives.*sep+*	{0: "NK", 1: "Sports", 2: "Market", 3: "IC"}
0.90625 *cls**sent_0*_You_are_currently_browsing_the*mask*_News_weblog_archives.*sep+*	{0: "Religion", 1: "Sports", 2: "Investment", 3: "Digital"}
0.90625 *cls**sent_0*_You_are_currently_browsing_the*mask*_News_weblog_archives.*sep+*	{0: "Africa", 1: "Sports", 2: "Market", 3: "Digital"}
0.90625 *cls**sent_0*_Read_the_full_story_on*mask*.com.*sep+*	{0: "military", 1: "Sports", 2: "industry", 3: "Tech"}
```
